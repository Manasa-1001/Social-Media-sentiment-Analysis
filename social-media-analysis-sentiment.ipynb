{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud  # For visualizing the text data more the word is frequent, larger the size it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\nikhil tanneeru\\appdata\\roaming\\python\\python312\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Nikhil\n",
      "[nltk_data]     Tanneeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Nikhil\n",
      "[nltk_data]     Tanneeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nikhil\n",
      "[nltk_data]     Tanneeru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns      #Seaborn is specifically designed for statistical data visualization, making it easier to create complex visualizations that show relationships in data.\n",
    "from colorama import Fore, init\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer   #Running -> run\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tqdm library is used to create progress bars in Python, making it easier to visualize the progress of loops, especially when processing large datasets.\n",
    "<br>\n",
    "The tqdm.notebook module is specifically designed for use in Jupyter Notebooks, ensuring that the progress bar is displayed properly within the notebook interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of vanderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.337, 'pos': 0.663, 'compound': 0.9497}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"I love this product! It's really amazing!!! ðŸ˜Š\"\n",
    "\n",
    "sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentimentdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>User</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 12:30:00</td>\n",
       "      <td>User123</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Nature #Park</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2023-01-15 08:45:00</td>\n",
       "      <td>CommuterX</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>#Traffic #Morning</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Just finished an amazing workout! ðŸ’ª          ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 15:45:00</td>\n",
       "      <td>FitnessFan</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Fitness #Workout</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2023-01-15 18:20:00</td>\n",
       "      <td>AdventureX</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>#Travel #Adventure</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>UK</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2023-01-15 19:55:00</td>\n",
       "      <td>ChefCook</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>#Cooking #Food</td>\n",
       "      <td>12.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                                Text    Sentiment  \\\n",
       "0   Enjoying a beautiful day at the park!        ...   Positive     \n",
       "1   Traffic was terrible this morning.           ...   Negative     \n",
       "2   Just finished an amazing workout! ðŸ’ª          ...   Positive     \n",
       "3   Excited about the upcoming weekend getaway!  ...   Positive     \n",
       "4   Trying out a new recipe for dinner tonight.  ...   Neutral      \n",
       "\n",
       "             Timestamp            User     Platform  \\\n",
       "0  2023-01-15 12:30:00   User123          Twitter     \n",
       "1  2023-01-15 08:45:00   CommuterX        Twitter     \n",
       "2  2023-01-15 15:45:00   FitnessFan      Instagram    \n",
       "3  2023-01-15 18:20:00   AdventureX       Facebook    \n",
       "4  2023-01-15 19:55:00   ChefCook        Instagram    \n",
       "\n",
       "                                     Hashtags  Retweets  Likes       Country  \\\n",
       "0   #Nature #Park                                  15.0   30.0     USA         \n",
       "1   #Traffic #Morning                               5.0   10.0     Canada      \n",
       "2   #Fitness #Workout                              20.0   40.0   USA           \n",
       "3   #Travel #Adventure                              8.0   15.0     UK          \n",
       "4   #Cooking #Food                                 12.0   25.0    Australia    \n",
       "\n",
       "   Year  Month  Day  Hour  \n",
       "0  2023      1   15    12  \n",
       "1  2023      1   15     8  \n",
       "2  2023      1   15    15  \n",
       "3  2023      1   15    18  \n",
       "4  2023      1   15    19  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3f12d_row0_col2, #T_3f12d_row0_col3, #T_3f12d_row1_col2, #T_3f12d_row1_col3, #T_3f12d_row2_col2, #T_3f12d_row2_col3, #T_3f12d_row3_col2, #T_3f12d_row3_col3, #T_3f12d_row4_col2, #T_3f12d_row4_col3, #T_3f12d_row5_col2, #T_3f12d_row5_col3, #T_3f12d_row6_col2, #T_3f12d_row6_col3, #T_3f12d_row7_col2, #T_3f12d_row7_col3, #T_3f12d_row8_col2, #T_3f12d_row8_col3, #T_3f12d_row9_col2, #T_3f12d_row9_col3, #T_3f12d_row10_col2, #T_3f12d_row10_col3, #T_3f12d_row11_col2, #T_3f12d_row11_col3, #T_3f12d_row12_col2, #T_3f12d_row12_col3, #T_3f12d_row13_col2, #T_3f12d_row13_col3, #T_3f12d_row14_col2, #T_3f12d_row14_col3 {\n",
       "  background-color: #8dd3c7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3f12d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3f12d_level0_col0\" class=\"col_heading level0 col0\" >features</th>\n",
       "      <th id=\"T_3f12d_level0_col1\" class=\"col_heading level0 col1\" >dtypes</th>\n",
       "      <th id=\"T_3f12d_level0_col2\" class=\"col_heading level0 col2\" >NaN count</th>\n",
       "      <th id=\"T_3f12d_level0_col3\" class=\"col_heading level0 col3\" >NaN percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3f12d_row0_col0\" class=\"data row0 col0\" >Unnamed: 0.1</td>\n",
       "      <td id=\"T_3f12d_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3f12d_row1_col0\" class=\"data row1 col0\" >Unnamed: 0</td>\n",
       "      <td id=\"T_3f12d_row1_col1\" class=\"data row1 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3f12d_row2_col0\" class=\"data row2 col0\" >Text</td>\n",
       "      <td id=\"T_3f12d_row2_col1\" class=\"data row2 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3f12d_row3_col0\" class=\"data row3 col0\" >Sentiment</td>\n",
       "      <td id=\"T_3f12d_row3_col1\" class=\"data row3 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3f12d_row4_col0\" class=\"data row4 col0\" >Timestamp</td>\n",
       "      <td id=\"T_3f12d_row4_col1\" class=\"data row4 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3f12d_row5_col0\" class=\"data row5 col0\" >User</td>\n",
       "      <td id=\"T_3f12d_row5_col1\" class=\"data row5 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3f12d_row6_col0\" class=\"data row6 col0\" >Platform</td>\n",
       "      <td id=\"T_3f12d_row6_col1\" class=\"data row6 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3f12d_row7_col0\" class=\"data row7 col0\" >Hashtags</td>\n",
       "      <td id=\"T_3f12d_row7_col1\" class=\"data row7 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3f12d_row8_col0\" class=\"data row8 col0\" >Retweets</td>\n",
       "      <td id=\"T_3f12d_row8_col1\" class=\"data row8 col1\" >float64</td>\n",
       "      <td id=\"T_3f12d_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3f12d_row9_col0\" class=\"data row9 col0\" >Likes</td>\n",
       "      <td id=\"T_3f12d_row9_col1\" class=\"data row9 col1\" >float64</td>\n",
       "      <td id=\"T_3f12d_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row9_col3\" class=\"data row9 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_3f12d_row10_col0\" class=\"data row10 col0\" >Country</td>\n",
       "      <td id=\"T_3f12d_row10_col1\" class=\"data row10 col1\" >object</td>\n",
       "      <td id=\"T_3f12d_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_3f12d_row11_col0\" class=\"data row11 col0\" >Year</td>\n",
       "      <td id=\"T_3f12d_row11_col1\" class=\"data row11 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row11_col3\" class=\"data row11 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_3f12d_row12_col0\" class=\"data row12 col0\" >Month</td>\n",
       "      <td id=\"T_3f12d_row12_col1\" class=\"data row12 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row12_col2\" class=\"data row12 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row12_col3\" class=\"data row12 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_3f12d_row13_col0\" class=\"data row13 col0\" >Day</td>\n",
       "      <td id=\"T_3f12d_row13_col1\" class=\"data row13 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row13_col2\" class=\"data row13 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row13_col3\" class=\"data row13 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3f12d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_3f12d_row14_col0\" class=\"data row14 col0\" >Hour</td>\n",
       "      <td id=\"T_3f12d_row14_col1\" class=\"data row14 col1\" >int64</td>\n",
       "      <td id=\"T_3f12d_row14_col2\" class=\"data row14 col2\" >0</td>\n",
       "      <td id=\"T_3f12d_row14_col3\" class=\"data row14 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x243e61aaa50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def null_count():\n",
    "    return pd.DataFrame({'features': df.columns,\n",
    "                'dtypes': df.dtypes.values,\n",
    "                'NaN count': df.isnull().sum().values,\n",
    "                'NaN percentage': df.isnull().sum().values/df.shape[0]}).style.background_gradient(cmap='Set3',low=0.1,high=0.01)\n",
    "null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Text', 'Sentiment', 'Timestamp', 'User',\n",
       "       'Platform', 'Hashtags', 'Retweets', 'Likes', 'Country', 'Year', 'Month',\n",
       "       'Day', 'Hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0.1: 732 distinct values\n",
      "Unnamed: 0: 732 distinct values\n",
      "Text: 707 distinct values\n",
      "Sentiment: 279 distinct values\n",
      "Timestamp: 683 distinct values\n",
      "User: 685 distinct values\n",
      "Platform: 4 distinct values\n",
      "Hashtags: 697 distinct values\n",
      "Retweets: 26 distinct values\n",
      "Likes: 38 distinct values\n",
      "Country: 115 distinct values\n",
      "Year: 14 distinct values\n",
      "Month: 12 distinct values\n",
      "Day: 31 distinct values\n",
      "Hour: 22 distinct values\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    num_distinct_values = len(df[column].unique())\n",
    "    print(f\"{column}: {num_distinct_values} distinct values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##          3. Feature Enginering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###         Drop Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Hashtags','Day', 'Hour','Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Timestamp', 'User', 'Platform', 'Retweets', 'Likes', 'Country',\n",
       "       'Year', 'Month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform\n",
       "Instagram     258\n",
       "Facebook      231\n",
       "Twitter       128\n",
       "Twitter       115\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Platform'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Twitter  \n",
       "1        Twitter  \n",
       "2       Instagram \n",
       "3        Facebook \n",
       "4       Instagram \n",
       "          ...     \n",
       "727      Facebook \n",
       "728     Instagram \n",
       "729       Twitter \n",
       "730      Facebook \n",
       "731     Instagram \n",
       "Name: Platform, Length: 732, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Platform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Platform'] = df['Platform'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "USA               59\n",
       "USA               55\n",
       "UK                49\n",
       "Canada            44\n",
       "Australia         41\n",
       "                  ..\n",
       "Netherlands        1\n",
       "USA                1\n",
       "Germany            1\n",
       "France             1\n",
       "USA                1\n",
       "Name: count, Length: 115, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = df['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       Timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "df['Day_of_Week'] = df['Timestamp'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Timestamp Day_of_Week\n",
      "0 2023-10-01 12:00:00      Sunday\n",
      "1 2023-10-02 13:30:00      Monday\n",
      "2 2023-10-03 09:45:00     Tuesday\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Timestamp': ['2023-10-01 12:00', '2023-10-02 13:30', '2023-10-03 09:45']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Extract day of the week\n",
    "df['Day_of_Week'] = df['Timestamp'].dt.day_name()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Month'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Month'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 18\u001b[0m\n\u001b[0;32m      2\u001b[0m month_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJanuary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFebruary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;241m12\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecember\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Map the month numbers to names\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(month_mapping)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert Month column to object type\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# df['Month'] = df['Month'].astype('object')\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Month'"
     ]
    }
   ],
   "source": [
    "# Month mapping\n",
    "month_mapping = {\n",
    "    1: 'January',\n",
    "    2: 'February',\n",
    "    3: 'March',\n",
    "    4: 'April',\n",
    "    5: 'May',\n",
    "    6: 'June',\n",
    "    7: 'July',\n",
    "    8: 'August',\n",
    "    9: 'September',\n",
    "    10: 'October',\n",
    "    11: 'November',\n",
    "    12: 'December'\n",
    "}\n",
    "\n",
    "# Map the month numbers to names\n",
    "df['Month'] = df['Month'].map(month_mapping)\n",
    "\n",
    "# Convert Month column to object type\n",
    "df['Month'] = df['Month'].astype('object')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##        Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_tokens)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text\n\u001b[1;32m---> 23\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean_Text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean_Text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  \n",
    "    text = \" \".join(text.split())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "   \n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df[\"Clean_Text\"] = df[\"Text\"].apply(clean)\n",
    "print(df[\"Clean_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing words to their root form or base form, known as the \"stem.\" The stem does not necessarily need to be a valid word; it simply represents the base meaning of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           enjoy beauti day park\n",
      "1            traffic terribl morn\n",
      "2             finish amaz workout\n",
      "3     excit upcom weekend getaway\n",
      "4    tri new recip dinner tonight\n",
      "Name: Clean_Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Text': [\n",
    "        'I enjoy a beautiful day at the park.',\n",
    "        'The traffic was terrible this morning.',\n",
    "        'I finished an amazing workout!',\n",
    "        'Excited for the upcoming weekend getaway!',\n",
    "        'Trying a new recipe for dinner tonight.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_ex = pd.DataFrame(data)\n",
    "\n",
    "# Initialize stemmer and stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  \n",
    "    text = \" \".join(text.split())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words]\n",
    "   \n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the text\n",
    "df_ex[\"Clean_Text\"] = df_ex[\"Text\"].apply(clean)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(df_ex[\"Clean_Text\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Unique Columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specified_columns = ['Platform','Country', 'Year','Month','Day_of_Week']\n",
    "\n",
    "for col in specified_columns:\n",
    "    total_unique_values = df[col].nunique()\n",
    "    print(f'Total unique values for {col}: {total_unique_values}')\n",
    "\n",
    "    top_values = df[col].value_counts()\n",
    "\n",
    "    colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE, Fore.LIGHTBLACK_EX, Fore.LIGHTRED_EX, Fore.LIGHTGREEN_EX]\n",
    "\n",
    "    for i, (value, count) in enumerate(top_values.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        print(f'{color}{value}: {count}{Fore.RESET}')\n",
    "\n",
    "    print('\\n' + '=' * 30 + '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (Main Part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df1['Vader_Score'] = df1['Clean_Text'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "df1['Sentiment'] = df1['Vader_Score'].apply(lambda score: 'positive' if score >= 0.05 else ('negative' if score <= -0.05 else 'neutral'))\n",
    "\n",
    "print(df1[['Clean_Text', 'Vader_Score', 'Sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#66b3ff', '#99ff99', '#ffcc99']\n",
    "\n",
    "explode = (0.1, 0, 0)  \n",
    "\n",
    "sentiment_counts = df1.groupby(\"Sentiment\").size()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    x=sentiment_counts, \n",
    "    labels=sentiment_counts.index,\n",
    "    autopct=lambda p: f'{p:.2f}%\\n({int(p*sum(sentiment_counts)/100)})', \n",
    "    wedgeprops=dict(width=0.7),\n",
    "    textprops=dict(size=10, color=\"r\"),  \n",
    "    pctdistance=0.7,\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    shadow=True)\n",
    "\n",
    "center_circle = plt.Circle((0, 0), 0.6, color='white', fc='white', linewidth=1.25)\n",
    "fig.gca().add_artist(center_circle)\n",
    "\n",
    "ax.text(0, 0, 'Sentiment\\nDistribution', ha='center', va='center', fontsize=14, fontweight='bold', color='#333333')\n",
    "\n",
    "ax.legend(sentiment_counts.index, title=\"Sentiment\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "ax.axis('equal')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##        Year Wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Year', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Years and Sentiment')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Month', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Month and Sentiment')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Day Of Weeek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Day_of_Week', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Day of Week and Sentiment')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Platform \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Platform', hue='Sentiment', data=df1, palette='Paired')\n",
    "plt.title('Relationship between Platform and Sentiment')\n",
    "plt.xlabel('Platform')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "top_10_countries = df1['Country'].value_counts().head(10).index\n",
    "\n",
    "df_top_10_countries = df1[df1['Country'].isin(top_10_countries)]\n",
    "\n",
    "sns.countplot(x='Country', hue='Sentiment', data=df_top_10_countries, palette='Paired')\n",
    "plt.title('Relationship between Country and Sentiment (Top 10 Countries)')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        4.2 Common Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['temp_list'] = df1['Clean_Text'].apply(lambda x: str(x).split())\n",
    "top_words = Counter([item for sublist in df1['temp_list'] for item in sublist])\n",
    "top_words_df = pd.DataFrame(top_words.most_common(20), columns=['Common_words', 'count'])\n",
    "\n",
    "top_words_df.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['temp_list'] = df1['Clean_Text'].apply(lambda x: str(x).split())\n",
    "top_words = Counter([item for sublist in df1['temp_list'] for item in sublist])\n",
    "top_words_df = pd.DataFrame(top_words.most_common(20), columns=['Common_words', 'count'])\n",
    "\n",
    "fig = px.bar(top_words_df,\n",
    "            x=\"count\",\n",
    "            y=\"Common_words\",\n",
    "            title='Common Words in Text Data',\n",
    "            orientation='h',\n",
    "            width=700,\n",
    "            height=700,\n",
    "            color='Common_words')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive_sent = df1[df1['Sentiment'] == 'positive']\n",
    "Negative_sent = df1[df1['Sentiment'] == 'negative']\n",
    "Neutral_sent = df1[df1['Sentiment'] == 'neutral']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Positive Common Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'positive']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([item for sublist in df1[df1['Sentiment'] == 'positive']['temp_list'] for item in sublist])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Neutral Common Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'neutral']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([item for sublist in df1[df1['Sentiment'] == 'neutral']['temp_list'] for item in sublist])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###          Negative Common Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = Counter([item for sublist in df1[df1['Sentiment'] == 'negative']['temp_list'] for item in sublist])\n",
    "temp_positive = pd.DataFrame(top.most_common(10), columns=['Common_words', 'count'])\n",
    "temp_positive.style.background_gradient(cmap='Reds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([item for sublist in df1[df1['Sentiment'] == 'negative']['temp_list'] for item in sublist])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###         5. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###         5.1 Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2['Clean_Text'].values\n",
    "y = df2['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random_state parameter in train_test_split controls the randomness of the data splitting process. When you set random_state=42, it makes sure that each time you run this code, the split between training and test sets will be exactly the same.\n",
    "Using random_state=42 is simply a convention (you could use any integer), but itâ€™s common in data science to use 42 as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###         6. Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer converts the text data into numerical values using the Term Frequency-Inverse Document Frequency (TF-IDF) technique, where frequently occurring words are weighted less, and unique terms are weighted higher.\n",
    "\n",
    "Setting max_features=5000 limits the vectorizer to the 5000 most significant features (words) in the text data.\n",
    "\n",
    "fit_transform is applied on the training data (X_train) to learn the vocabulary and transform it into a sparse matrix representation. transform is applied on the test data (X_test) to convert it into the same format as X_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Passive Aggressive Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_classifier = PassiveAggressiveClassifier(max_iter=50, random_state=42)\n",
    "pac_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pac_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# The predict() method calculates a score for each possible class (such as \"positive\" or \"negative\" sentiment) for each document. \n",
    "# It assigns the label that has the highest score as the predicted class for that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "classification_rep_test = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_test}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###        Logistic Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier = LogisticRegression(max_iter=50, random_state=42)\n",
    "logistic_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = logistic_classifier.predict(X_test_tfidf)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "classification_rep_logistic = classification_report(y_test, y_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_logistic}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Random Fores Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "random_forest_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = random_forest_classifier.predict(X_test_tfidf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_rep_rf = classification_report(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     SVM Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_classifier.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support Vector Machine Results:\")\n",
    "print(f\"Accuracy: {accuracy_svm}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###      Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "classification_rep_nb = classification_report(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMultinomial Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#         Best Modeling : Passive Aggressive Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'C': [0.1, 0.5, 1.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    'shuffle': [True, False],\n",
    "    'verbose': [0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_classifier = PassiveAggressiveClassifier(random_state=42)\n",
    "\n",
    "randomized_search = RandomizedSearchCV(pac_classifier, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "randomized_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_randomized = randomized_search.best_params_\n",
    "best_params_randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pac_classifier_randomized = PassiveAggressiveClassifier(random_state=42, **best_params_randomized)\n",
    "best_pac_classifier_randomized.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best_pac_randomized = best_pac_classifier_randomized.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_best_pac_randomized = accuracy_score(y_test, y_pred_best_pac_randomized)\n",
    "classification_rep_best_pac_randomized = classification_report(y_test, y_pred_best_pac_randomized)\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_best_pac_randomized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best PassiveAggressiveClassifier Model (RandomizedSearchCV):\")\n",
    "print(f\"Best Hyperparameters: {best_params_randomized}\")\n",
    "print(f\"Accuracy: {accuracy_best_pac_randomized}\")\n",
    "print(\"Classification Report:\\n\", classification_rep_best_pac_randomized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Greys', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.title('Confusion Matrix - Hyperparameters')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4245661,
     "sourceId": 7316566,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
